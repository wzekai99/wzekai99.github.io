<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    
    <!-- Website verification -->
    <meta name="google-site-verification" content="bJifYeU5_78wrxhSyC5O7CWMNJIJMgxFyGeVFxrzJ-s">
<meta name="msvalidate.01" content="0F47E8F5E819CF25475BA844139C6069">
<!-- Avoid warning on Google Chrome
        Error with Permissions-Policy header: Origin trial controlled feature not enabled: 'interest-cohort'.
        see https://stackoverflow.com/a/75119417
    -->
    <meta http-equiv="Permissions-Policy" content="interest-cohort=()">

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Zekai  Wang</title>
    <meta name="author" content="Zekai  Wang">
    <meta name="description" content="My name is Zekai Wang (王泽铠). I am an M.S. student in the School of Computer Science at Wuhan University, advised by Prof. Weiwei Liu. My research primarily revolves around machine learning, specifically emphasizing trustworthy machine learning and multi-task learning.
">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%92%A1&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://zekai.site/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <!-- Social Icons -->
          <div class="navbar-brand social">
            <a href="mailto:%77%7A%65%6B%61%69%39%39@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fas fa-envelope"></i></a>
            <a href="https://scholar.google.com/citations?user=EU9-95cAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a>
            <a href="https://github.com/wzekai99" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a>
            <a href="https://twitter.com/wzekai99" title="Twitter" rel="external nofollow noopener" target="_blank"><i class="fab fa-twitter"></i></a>
            

          </div>
          
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item active">
                <a class="nav-link" href="/">About<span class="sr-only">(current)</span></a>
              </li>
              

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">Publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">CV</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- about.html -->
      <div class="post">
        <header class="post-header">
          <h1 class="post-title">
           <span class="font-weight-bold">Zekai</span>  Wang
          </h1>
          <p class="desc"><strong>王 泽铠</strong>, M.S. Student at <a href="https://en.whu.edu.cn" rel="external nofollow noopener" target="_blank">Wuhan University</a></p>
        </header>

        <article>
          <div class="profile float-right">

              <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/prof_pic-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/prof_pic-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/prof_pic-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/prof_pic.jpg" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="prof_pic.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

          </div>

          <div class="clearfix">
            <p>I am an M.S. student in the School of Computer Science at Wuhan University, advised by Prof. <a href="https://sites.google.com/site/weiweiliuhomepage" rel="external nofollow noopener" target="_blank">Weiwei Liu</a>. Before that, I received my B.S. degree in computer science from Wuhan University in 2021. I was a visiting student at the University of California, Berkeley from January 2020 to June 2020. I interned at Sea AI Lab, Singapore from September 2022 to June 2023.</p>

<p>My research primarily revolves around machine learning, specifically emphasizing trustworthy machine learning and multi-task learning.</p>

<ul>
  <li>
<strong>Trustworthy:</strong> <a href="https://proceedings.mlr.press/v202/wang23ad.html" rel="external nofollow noopener" target="_blank">ICML’23</a>, <a href="https://papers.nips.cc/paper_files/paper/2022/hash/a80ebbb4ec9e9b39789318a0a61e2e43-Abstract-Conference.html" rel="external nofollow noopener" target="_blank">NeurIPS’22</a>, <a href="https://proceedings.mlr.press/v162/wang22q.html" rel="external nofollow noopener" target="_blank">ICML’22</a>
</li>
  <li>
<strong>Multi-task learning in NLP:</strong> <a href="https://ieeexplore.ieee.org/abstract/document/9893398" rel="external nofollow noopener" target="_blank">TKDE</a>, <a href="https://aclanthology.org/2022.findings-acl.271" rel="external nofollow noopener" target="_blank">ACL findings’22</a>, <a href="https://aclanthology.org/2021.acl-long.428" rel="external nofollow noopener" target="_blank">ACL’21</a>
</li>
</ul>


          </div>

          <!-- News -->
          

          <!-- Latest posts -->
          

          <!-- Selected papers -->
          <h2>Selected Publications <a href="/publications/">[full list]</a>
</h2>(* indicates equal contribution)
          <div class="publications">
            <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">ICML’23</abbr></div>

        <!-- Entry bib key -->
        <div id="wang2023better" class="col-sm-8">
        <!-- Title --><div class="title"><a href="https://proceedings.mlr.press/v202/wang23ad/wang23ad.pdf" rel="external nofollow noopener" target="_blank">Better Diffusion Models Further Improve Adversarial Training</a></div>
<!-- Author -->
        <div class="author">
        

        <strong><u>Zekai Wang</u></strong>*, <a href="https://p2333.github.io" rel="external nofollow noopener" target="_blank">Tianyu Pang</a>*, <a href="https://duchao0726.github.io" rel="external nofollow noopener" target="_blank">Chao Du</a>, <a href="https://linmin.me" rel="external nofollow noopener" target="_blank">Min Lin</a>, <a href="https://sites.google.com/site/weiweiliuhomepage" rel="external nofollow noopener" target="_blank">Weiwei Liu</a>, and <a href="https://yanshuicheng.info" rel="external nofollow noopener" target="_blank">Shuicheng Yan</a>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In International Conference on Machine Learning</em>, 2023
        </div>
        <div class="periodical">
          
        </div>
<span style="color: var(--global-theme-color); font-weight: bolder">New state-of-the-art performance on the <a href="https://robustbench.github.io" rel="external nofollow noopener" target="_blank">RobustBench leaderboard</a></span><!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn z-depth-0" role="button">Abstract</a>
            <a href="http://arxiv.org/abs/2302.04638" class="btn z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
            <a class="bibtex btn z-depth-0" role="button">Bib</a>
            <a href="https://proceedings.mlr.press/v202/wang23ad.html" class="btn z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a>
            <a href="https://github.com/wzekai99/DM-Improves-AT" class="btn z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
            <a href="https://icml.cc/virtual/2023/poster/24423" class="btn z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Poster</a>
          </div>
          
          <div class="badges">
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>It has been recognized that the data generated by the denoising diffusion probabilistic model (DDPM) improves adversarial training. After two years of rapid development in diffusion models, a question naturally arises: can better diffusion models further improve adversarial training? This paper gives an affirmative answer by employing the most recent diffusion model which has higher efficiency (∼20 sampling steps) and image quality (lower FID score) compared with DDPM. Our adversarially trained models achieve state-of-the-art performance on RobustBench using only generated data (no external datasets). Under the L∞-norm threat model with ε=8/255, our models achieve 70.69% and 42.67% robust accuracy on CIFAR-10 and CIFAR-100, respectively, i.e. improving upon previous state-of-the-art models by +4.58% and +8.03%. Under the L2-norm threat model with ε=128/255, our models achieve 84.86% on CIFAR-10 (+4.44%). These results also beat previous works that use external data. We also provide compelling results on the SVHN and TinyImageNet datasets.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">wang2023better</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Better Diffusion Models Further Improve Adversarial Training}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wang, Zekai and Pang, Tianyu and Du, Chao and Lin, Min and Liu, Weiwei and Yan, Shuicheng}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Machine Learning}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{202}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{36246--36263}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr">
<abbr class="badge">ICML’22</abbr><abbr class="badge badge-light">Oral</abbr>
</div>

        <!-- Entry bib key -->
        <div id="wang2022robustness" class="col-sm-8">
        <!-- Title --><div class="title"><a href="https://proceedings.mlr.press/v162/wang22q/wang22q.pdf" rel="external nofollow noopener" target="_blank">Robustness Verification for Contrastive Learning</a></div>
<!-- Author -->
        <div class="author">
        

        <strong><u>Zekai Wang</u></strong>, and <a href="https://sites.google.com/site/weiweiliuhomepage" rel="external nofollow noopener" target="_blank">Weiwei Liu</a>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In International Conference on Machine Learning</em>, 2022
        </div>
        <div class="periodical">
          
        </div>
<span style="color: var(--global-theme-color); font-weight: bolder">Long Presentation (top 2%)</span><!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn z-depth-0" role="button">Abstract</a>
            <a class="bibtex btn z-depth-0" role="button">Bib</a>
            <a href="https://proceedings.mlr.press/v162/wang22q.html" class="btn z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a>
            <a href="https://icml.cc/virtual/2022/oral/16762" class="btn z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a>
            <a href="https://github.com/wzekai99/RVCL" class="btn z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
            <a href="https://icml.cc/virtual/2022/poster/16761" class="btn z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Poster</a>
          </div>
          
          <div class="badges">
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Contrastive adversarial training has successfully improved the robustness of contrastive learning (CL). However, the robustness metric used in these methods is linked to attack algorithms, image labels and downstream tasks, all of which may affect the consistency and reliability of robustness metric for CL. To address these problems, this paper proposes a novel Robustness Verification framework for Contrastive Learning (RVCL). Furthermore, we use extreme value theory to reveal the relationship between the robust radius of the CL encoder and that of the supervised downstream task. Extensive experimental results on various benchmark models and datasets verify our theoretical findings, and further demonstrate that our proposed RVCL is able to evaluate the robustness of both models and images.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">wang2022robustness</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Robustness Verification for Contrastive Learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wang, Zekai and Liu, Weiwei}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Machine Learning}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{162}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{22865--22883}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">AAAI’24</abbr></div>

        <!-- Entry bib key -->
        <div id="wang2024drf" class="col-sm-8">
        <!-- Title --><div class="title">DRF: Improving Certified Robustness via Distributional Robustness Framework</div>
<!-- Author -->
        <div class="author">
        

        <strong><u>Zekai Wang</u></strong>, Zhengyu Zhou, and <a href="https://sites.google.com/site/weiweiliuhomepage" rel="external nofollow noopener" target="_blank">Weiwei Liu</a>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In AAAI Conference on Artificial Intelligence</em>, 2024
        </div>
        <div class="periodical">
          
        </div>
<!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn z-depth-0" role="button">Abstract</a>
            <a class="bibtex btn z-depth-0" role="button">Bib</a>
          </div>
          
          <div class="badges">
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Randomized smoothing (RS) provides state-of-the-art (SOTA) certified robustness against L2-perturbations for large neural networks. Among studies in this field, methods based on adversarial training (AT) achieve remarkably robust performance by applying adversarial examples to construct the smoothed classifier. These AT-based RS methods typically seek a pointwise adversary that generates the worst-case adversarial examples by perturbing each input independently. However, there are unexplored benefits to considering such adversarial robustness across the entire data distribution. To this end, we provide a novel framework called DRF, which connects AT-based RS methods with distributional robustness (DR), and show that these methods are special cases of their counterparts in our framework. Due to the advantages conferred by DR, our framework can control the trade-off between the clean accuracy and certified robustness of smoothed classifiers to a significant extent. Our experiments demonstrate that DRF can substantially improve the certified L2-robustness of AT-based RS.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">wang2024drf</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{DRF: Improving Certified Robustness via Distributional Robustness Framework}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wang, Zekai and Zhou, Zhengyu and Liu, Weiwei}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{AAAI Conference on Artificial Intelligence}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">NeurIPS’22</abbr></div>

        <!-- Entry bib key -->
        <div id="ma2022tradeoff" class="col-sm-8">
        <!-- Title --><div class="title"><a href="https://papers.nips.cc/paper_files/paper/2022/file/a80ebbb4ec9e9b39789318a0a61e2e43-Paper-Conference.pdf" rel="external nofollow noopener" target="_blank">On the Tradeoff Between Robustness and Fairness</a></div>
<!-- Author -->
        <div class="author">
        

        Xinsong Ma, <strong><u>Zekai Wang</u></strong>, and <a href="https://sites.google.com/site/weiweiliuhomepage" rel="external nofollow noopener" target="_blank">Weiwei Liu</a>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Advances in Neural Information Processing Systems</em>, 2022
        </div>
        <div class="periodical">
          
        </div>
<!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn z-depth-0" role="button">Abstract</a>
            <a class="bibtex btn z-depth-0" role="button">Bib</a>
            <a href="https://papers.nips.cc/paper_files/paper/2022/hash/a80ebbb4ec9e9b39789318a0a61e2e43-Abstract-Conference.html" class="btn z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a>
            <a href="https://github.com/wzekai99/FAT" class="btn z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
          
          <div class="badges">
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Interestingly, recent experimental results have identified a robust fairness phenomenon in adversarial training (AT), namely that a robust model well-trained by AT exhibits a remarkable disparity of standard accuracy and robust accuracy among different classes compared with natural training. However, the effect of different perturbation radii in AT on robust fairness has not been studied, and one natural question is raised: does a tradeoff exist between average robustness and robust fairness? Our extensive experimental results provide an affirmative answer to this question: with an increasing perturbation radius, stronger AT will lead to a larger class-wise disparity of robust accuracy. Theoretically, we analyze the class-wise performance of adversarially trained linear models with mixture Gaussian distribution. Our theoretical results support our observations. Moreover, our theory shows that adversarial training easily leads to more serious robust fairness issue than natural training. Motivated by theoretical results, we propose a fairly adversarial training (FAT) method to mitigate the tradeoff between average robustness and robust fairness. Experimental results validate the effectiveness of our proposed method.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ma2022tradeoff</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{On the Tradeoff Between Robustness and Fairness}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ma, Xinsong and Wang, Zekai and Liu, Weiwei}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Advances in Neural Information Processing Systems}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{35}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{26230--26241}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">TKDE</abbr></div>

        <!-- Entry bib key -->
        <div id="mao2023task" class="col-sm-8">
        <!-- Title --><div class="title">Task Variance Regularized Multi-Task Learning</div>
<!-- Author -->
        <div class="author">
        

        Yuren Mao, <strong><u>Zekai Wang</u></strong>, <a href="https://sites.google.com/site/weiweiliuhomepage" rel="external nofollow noopener" target="_blank">Weiwei Liu</a>, Xuemin Lin, and Wenbin Hu</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>IEEE Transactions on Knowledge and Data Engineering</em>, 2023
        </div>
        <div class="periodical">
          
        </div>
<!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn z-depth-0" role="button">Abstract</a>
            <a class="bibtex btn z-depth-0" role="button">Bib</a>
            <a href="https://ieeexplore.ieee.org/abstract/document/9893398" class="btn z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a>
          </div>
          
          <div class="badges">
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Multi-task Learning (MTL), which involves the simultaneous learning of multiple tasks, can achieve better performance than learning each task independently. It has achieved great success in various applications, ranging from Computer Vision (CV) to Natural Language Processing (NLP). In MTL, the losses of the including tasks are jointly optimized. However, it is common for these tasks to be competing. When the tasks are competing, minimizing the losses of some tasks increases the losses of others, which accordingly increases the task variance (variance between the task-specific loss); furthermore, it induces under-fitting in some tasks and over-fitting in others, which degenerates the generalization performance of an MTL model. To address this issue, it is necessary to control the task variance; thus, task variance regularization is a natural choice. While intuitive, task variance regularization remains unexplored in MTL. Accordingly, to fill this gap, we study the generalization error bound of MTL through the lens of task variance and propose the task variance matters the generalization performance of MTL. Furthermore, this paper investigates how the task variance might be effectively regularized, and consequently proposes a multi-task learning method based on adversarial multi-armed bandit. The proposed method, dubbed BanditMTL, regularizes the task variance by means of a mirror gradient ascent-descent algorithm. Adopting BanditMTL both in CV and NLP applications is found to achieve state-of-the-art performance. The results of extensive experiments back up our theoretical analysis and validate the superiority of our proposals.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">mao2023task</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Task Variance Regularized Multi-Task Learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mao, Yuren and Wang, Zekai and Liu, Weiwei and Lin, Xuemin and Hu, Wenbin}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Knowledge and Data Engineering}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{35}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{8}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{8615-8629}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">ACL’21</abbr></div>

        <!-- Entry bib key -->
        <div id="mao2021banditmtl" class="col-sm-8">
        <!-- Title --><div class="title"><a href="https://aclanthology.org/2021.acl-long.428.pdf" rel="external nofollow noopener" target="_blank">BanditMTL: Bandit-based Multi-task Learning for Text Classification</a></div>
<!-- Author -->
        <div class="author">
        

        Yuren Mao, <strong><u>Zekai Wang</u></strong>, <a href="https://sites.google.com/site/weiweiliuhomepage" rel="external nofollow noopener" target="_blank">Weiwei Liu</a>, Xuemin Lin, and Wenbin Hu</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Annual Meeting of the Association for Computational Linguistics</em>, 2021
        </div>
        <div class="periodical">
          
        </div>
<!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn z-depth-0" role="button">Abstract</a>
            <a class="bibtex btn z-depth-0" role="button">Bib</a>
            <a href="https://aclanthology.org/2021.acl-long.428" class="btn z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a>
          </div>
          
          <div class="badges">
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Task variance regularization, which can be used to improve the generalization of Multi-task Learning (MTL) models, remains unexplored in multi-task text classification. Accordingly, to fill this gap, this paper investigates how the task might be effectively regularized, and consequently proposes a multi-task learning method based on adversarial multi-armed bandit. The proposed method, named BanditMTL, regularizes the task variance by means of a mirror gradient ascent-descent algorithm. Adopting BanditMTL in the multi-task text classification context is found to achieve state-of-the-art performance. The results of extensive experiments back up our theoretical analysis and validate the superiority of our proposals.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">mao2021banditmtl</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{BanditMTL: Bandit-based Multi-task Learning for Text Classification}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mao, Yuren and Wang, Zekai and Liu, Weiwei and Lin, Xuemin and Hu, Wenbin}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Annual Meeting of the Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{5506--5516}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
</ol>
          </div>


          <!-- Social -->
        </article>

</div>

      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2023 Zekai  Wang. Last updated: December 09, 2023.
      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js"></script>
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>

    

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-2W5EM1MVTS"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-2W5EM1MVTS');
  </script>
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
